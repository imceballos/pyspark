# -*- coding: utf-8 -*-
"""Adstra_challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UYBSPmCK3OcCXyPxSoDbpkIb7IuCk4MK

## Challenge

The deliverable is generated by the use of multiple function called into *generate_report*, this function contains the following:

- common_columns: Return the commons columns of two DataFrames
- different_columns: Return the different columns of two DataFrames
- shape_diff: Return the Dimensional Difference, this is the rows and columns count difference
- rows_similar_diff: Get the number of common and different rows and entries between two dataframes.
- count_occurrences: Count the number the elements (HumanId and TrustRate) 
- get_all_ocurrences: Join two dataframes generated by the count_occurences, to display the count numbers.

There is defined the TrustRate and HumanId change as a rate between the diff of the change counter divided by the total number of entries.
"""


def common_columns(col1, col2):
  return [x for x in col1 if x in col2]

def different_columns(col1, col2):
  common_col = common_columns(col1, col2)
  return set(col1).union(set(col2)) - set(common_col)

def shape_diff(df1, df2):
  rows_diff = abs(df2.count() - df1.count())
  cols_diff = abs(len(df2.columns) - len(df1.columns))
  return f"Rows: {rows_diff} | Cols: {cols_diff}"

def rows_similar_diff(dfA, dfB):
  cmon_columns = common_columns(dfA.columns, dfB.columns)
  common_rows = dfA[cmon_columns].intersectAll(dfB[cmon_columns])
  dfA_not_dfB = dfA[cmon_columns].exceptAll(dfB[cmon_columns])
  dfB_not_dfA = dfB[cmon_columns].exceptAll(dfA[cmon_columns])
  n_diff_rows = dfA_not_dfB.count()+dfB_not_dfA.count()
  return f"{common_rows.count()} common rows, {common_rows.count()*len(cmon_columns)} elements \
  | {n_diff_rows} different rows, {n_diff_rows*len(cmon_columns)} elements"

def count_occurences(dfA, field, colname):
  return dfA.groupBy(field).count().select(field, F.col('count').alias(f'n_{colname}')).orderBy(F.col(f'n_{colname}').desc()).fillna("0")

def get_all_ocurrences(dfA, dfB, field, idA, idB):
  df_A = count_occurences(dfA, field, idA)
  df_B = count_occurences(dfB, field, idB)
  return df_A.join(df_B,[field],how='full').fillna(0)

import pyspark.sql.functions  as F 
from pyspark.sql import SparkSession

def generate_report(dfA, dfB, idA, idB):
    cmon_cols = "|".join(common_columns(dfA.columns, dfB.columns))
    diff_cols = "|".join(different_columns(dfA.columns, dfB.columns))
    diff_shape = shape_diff(dfA, dfB)
    trustrate_count = get_all_ocurrences(dfA, dfB, "TrustRate", idA, idB)
    humanid_count = get_all_ocurrences(dfA, dfB, "HumanId", idA, idB)

    print(f"Common columns df{idA} & df{idB}:    {cmon_cols} \n")
    print(f"Different columns df{idA} & df{idB}: {diff_cols} \n")
    print(f"Comparison data:             {rows_similar_diff(dfA, dfB)}")
  
    print(f"Dimensional difference:      {diff_shape} \n")

    print("TrustRate count")
    trustrate_count.show()

    print("HumanId count")
    humanid_count.show()

    trustrate_count = trustrate_count.withColumn('abs_diff', F.abs(F.col(f'n_{idA}') - F.col(f'n_{idB}')))
    humanid_count = humanid_count.withColumn('abs_diff', F.abs(F.col(f'n_{idA}') - F.col(f'n_{idB}')))

    trustrate_count = trustrate_count.withColumn('max_value', F.greatest(F.col(f'n_{idA}'), F.col(f'n_{idB}')))
    humanid_count = humanid_count.withColumn('max_value', F.greatest(F.col(f'n_{idA}'),  F.col(f'n_{idB}')))

    trustrate_change = round(trustrate_count.select(F.sum('abs_diff')).collect()[0][0]/trustrate_count.select(F.sum('max_value')).collect()[0][0],2)
    humanid_change = round(humanid_count.select(F.sum('abs_diff')).collect()[0][0]/humanid_count.select(F.sum('max_value')).collect()[0][0],2)

    avg_change_rate = (trustrate_change +humanid_change)/2

    if avg_change_rate < 0.15:
      print("There are not important changes in the data: ")
      print(f"TrustRate change: {trustrate_change}")
      print(f"HumanId change:   {humanid_change}")

    elif avg_change_rate > 0.15 and avg_change_rate < 0.3 :
      print("There are not major changes in the data")
      print(f"TrustRate change: {trustrate_change}")
      print(f"HumanId change:   {humanid_change}")
    else:
      print("There are critical changes in the data")
      print(f"TrustRate change: {trustrate_change}")
      print(f"HumanId change:   {humanid_change}")

spark = SparkSession.builder.appName("Read Parquet Files").getOrCreate()

dfA = spark.read.option("header", "true").option("inferSchema", "true").option("numPartitions", "4").parquet("resultsA.parquet")
dfB = spark.read.option("header", "true").option("inferSchema", "true").option("numPartitions", "4").parquet("resultsB.parquet")
dfC = spark.read.option("header", "true").option("inferSchema", "true").option("numPartitions", "4").parquet("resultsC.parquet")

generate_report(dfA, dfB, "A", "B")

generate_report(dfA, dfC, "A", "C")